We investigate online learning problems with bandit feedback. With non-convex function assumptions, we provide theoretical analysis for dynamic regret. The regret depends heavily on how rapidly function sequence changes. Inspired by this finding, we propose two algorithms, Empirical Policy Gradient with Uniform Exploration (PGE), and Empirical Logit Learning with $\varepsilon$-Greedy Exploration (LLE) for stochastic bandit setting. PGE satisfies sub-linear regret, while LLE achieves nearly optimal regret. In each proposed approach, the agent maintains its action selection strategy as a parametric model. With sufficient exploration, a neural network is trained to minimize an empirically estimated policy or value based loss using gradient updates. A policy is then recovered by exponentiating and renormalizing the learned logits. These results can be generalized to episodic Markov decision processes and the state dependent bandit cases. Experimental simulations verify our theoretical analyses.

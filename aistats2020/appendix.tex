\section{Appendix}

\subsection{Proof for \cref{thm:online_learning_regret}}

\textbf{\cref{thm:online_learning_regret}.} For any sequence of functions $\left\{ f_t : t = 0, 1, \dots, T \right\}$ that satisfies \cref{asmp:boundedness_smoothness} and \cref{asmp:gradient_dominant}, dynamic regret of online gradient update $\rvx_{t+1} \gets \rvx_{t} - \eta \nabla f_{t}(\rvx_{t})$ with $\eta = 1/L$ satisfies,
\begin{equation}
\label{eq:online_learning_regret}
\begin{split}
    \sum\limits_{t=0}^{T-1}{\left[ f_t(\rvx_t) - f_t( \rvx_t^*) \right]} \le \frac{2}{G} \sqrt{L T ( 1 + V_T^f )},
\end{split}
\end{equation}
where $\rvx_t^* \coloneqq \argmin_{\rvw}{f_t(\rvw)}$, $\forall t$.

\begin{proof}
First, note that $\left\| \nabla f_t(\rvx ) - \nabla f_t(\rvy) \right\| \le L $ in \cref{asmp:boundedness_smoothness} is equivalent with
\begin{equation*}
\begin{split}
    f_t(\rvx^\prime) \le f_t(\rvx) + \left\langle \nabla f_t(\rvx) , \rvx^\prime - \rvx \right\rangle +  \frac{L}{2} \left\| \rvx^\prime - \rvx \right\|^2, \quad \forall \rvx, \ \rvx^\prime.
\end{split}
\end{equation*}
Let $\rvx^\prime = \rvx_{t+1}$, $\rvx = \rvx_{t}$, and use $\eta = 1/L$,
\begin{equation}
\label{eq:f_t_progress}
\begin{split}
    f_t(\rvx_{t+1}) &\le f_t(\rvx_{t}) + \left\langle \nabla f_t(\rvx_t) , - \eta \nabla f_t(\rvx_t) \right\rangle +  \frac{L \eta^2}{2} \left\| \nabla f_t(\rvx_t) \right\|^2 \\
    &= f_t(\rvx_{t}) - \frac{1}{2 L} \left\| \nabla f_t(\rvx_t) \right\|^2 \\
    &\le f_t(\rvx_{t}) - \frac{G^2}{2 L} \left[ f_t(\rvx_t) - f_t(\rvx_t^*) \right]^2,
\end{split}
\end{equation}
where the last inequality is according to \cref{asmp:gradient_dominant}. Denote $\delta_t \coloneqq f_t(\rvx_t) - f_t( \rvx_t^* )$, $\forall t$.
\begin{equation*}
\begin{split}
    \delta_t &= f_t(\rvx_t) - f_{t-1}(\rvx_t) + f_{t-1}(\rvx_t) - f_t( \rvx_t^* ) \\
    &\le \max\limits_{\rvw}{\left| f_{t}(\rvw) - f_{t-1}(\rvw) \right| } + f_{t-1}(\rvx_t) - f_t( \rvx_t^* ) \\
    &= \max\limits_{\rvw}{\left| f_{t}(\rvw) - f_{t-1}(\rvw) \right| } + f_{t-1}(\rvx_t) - f_{t-1}(\rvx_{t-1}) + f_{t-1}(\rvx_{t-1}) - f_t( \rvx_t^* ) \\
    &\le \max\limits_{\rvw}{\left| f_{t}(\rvw) - f_{t-1}(\rvw) \right| } - \frac{G^2}{2L} \delta_{t-1}^2 + f_{t-1}(\rvx_{t-1}) - f_t( \rvx_t^*) \qquad \left( \text{by \cref{eq:f_t_progress}} \right) \\
    &=  \max\limits_{\rvw}{\left| f_{t}(\rvw) - f_{t-1}(\rvw) \right| } - \frac{G^2}{2L} \delta_{t-1}^2 + f_{t-1}(\rvx_{t-1}) - f_{t-1}(\rvx_{t-1}^*) + f_{t-1}(\rvx_{t-1}^*) - f_t( \rvx_t^* ) \\
    &= \max\limits_{\rvw}{\left| f_{t}(\rvw) - f_{t-1}(\rvw) \right| } - \frac{G^2}{2L} \delta_{t-1}^2 + \delta_{t-1} + f_{t-1}(\rvx_{t-1}^*) - f_t( \rvx_t^*) \\
    &= \max\limits_{\rvw}{\left| f_{t}(\rvw) - f_{t-1}(\rvw) \right| } - \frac{G^2}{2L} \delta_{t-1}^2 + \delta_{t-1} + f_{t-1}(\rvx_{t-1}^*) - f_{t-1}(\rvx_{t}^*) + f_{t-1}(\rvx_{t}^*) - f_t( \rvx_t^*) \\
    &\le \max\limits_{\rvw}{\left| f_{t}(\rvw) - f_{t-1}(\rvw) \right| } - \frac{G^2}{2L} \delta_{t-1}^2 + \delta_{t-1} + f_{t-1}(\rvx_{t}^*) - f_t( \rvx_t^*) \qquad \left(f_{t-1}(\rvx_{t-1}^*) \coloneqq \min\limits_{\rvw}{f_{t-1}(\rvw)} \le f_{t-1}(\rvx_t^*) \right) \\
    &\le \max\limits_{\rvw}{\left| f_{t}(\rvw) - f_{t-1}(\rvw) \right| } - \frac{G^2}{2L} \delta_{t-1}^2 + \delta_{t-1} + \max\limits_{\rvw}{\left| f_{t}(\rvw) - f_{t-1}(\rvw) \right| } \\
    &= - \frac{G^2}{2L} \delta_{t-1}^2 + \delta_{t-1} + 2 \max\limits_{\rvw}{\left| f_{t}(\rvw) - f_{t-1}(\rvw) \right| }.
\end{split}
\end{equation*}
Rearranging and summing up from $1$ to $T$,
\begin{equation}
\label{eq:delta_t_upper_bound}
\begin{split}
    \frac{G^2}{2L} \cdot \sum\limits_{t=1}^{T}{ \delta_{t-1}^2} &\le \sum\limits_{t=1}^{T}{\left[ \delta_{t-1} - \delta_{t} + 2 \max\limits_{\rvw}{\left| f_{t}(\rvw) - f_{t-1}(\rvw) \right|} \right] } \\
    &= \sum\limits_{t=1}^{T}{ \left[ \delta_{t-1} - \delta_{t} \right] } + 2 \sum\limits_{t=0}^{T-1}{ \max\limits_{\rvw}{\left| f_{t+1}(\rvw) - f_{t}(\rvw) \right| } } \\
    &= \delta_0 - \delta_{T} + 2 V_T^f \qquad \left( \text{\cref{defi:function_variation}} \right) \\
    &\le 2 + 2 V_T^f \qquad \left(\text{\cref{asmp:boundedness_smoothness}}\right).
\end{split}
\end{equation}
According to the Root-Mean Square-Arithmetic Mean inequality, the regret is upper bounded as
\begin{equation*}
\begin{split}
    \sum\limits_{t=0}^{T-1}{\left[ f_t(\rvx_t) - f_t( \rvx_t^*) \right]} = \sum\limits_{t=1}^{T}{ \delta_{t-1}} &\le \sqrt{T \sum\limits_{t=1}^{T}{ \delta_{t-1}^2}} \\
    &\le \frac{2}{G} \sqrt{L T ( 1 + V_T^f)}. \qquad \left(\text{by \cref{eq:delta_t_upper_bound}}\right) \qedhere
\end{split}
\end{equation*}
\end{proof}

\subsection{Proof for \cref{thm:policy_gradient_main_result} }

\textbf{\cref{thm:policy_gradient_main_result}.}
	Given policy neural networks as shown in \cref{fig:nn_policy_value}, with number of parameters $m \ge T^2 / K^2$, learning rate $\eta = \frac{1}{2 K m}$, $\beta = \frac{ \ln{(K/3) + \ln{\ln{t}} } }{ 3 \ln{t}}$, $\forall t \ge 2$. The expected regret of \cref{alg:policy_gradient_uniform_exploration} satisfies,
	\begin{equation*}
	\begin{split}
	\sum\limits_{t=0}^{T-1}{ ( \rvpi^* - \tilde{\rvpi}_t )^\top \rvr} \le O(T^{\frac{2}{3}}( \ln{T})^{\frac{1}{3}}).
	\end{split}
	\end{equation*}

\begin{proof}
    According to \cref{alg:policy_gradient_uniform_exploration}, $\tilde{\rvpi}_t$ consists of $\rvpi_t$ mixed with decaying uniform denoted as $\bar{\rvpi}_t$. The regret is decomposed by linearity of expectation.
\begin{equation}
\label{eq:total_regret_decomposition_appendix}
\begin{split}
    \sum\limits_{t=0}^{T-1}{ ( {\rvpi^*} - \tilde{\rvpi}_t )^\top \rvr } &\le 1 + \sum\limits_{t=1}^{T-1}{ \left[ \left( 1 - t^{ \beta - \frac{1}{3}} \right) \cdot \left( {\rvpi^*} - \rvpi_t \right) + t^{ \beta - \frac{1}{3}} \cdot ( {\rvpi^*} - \bar{\rvpi}_t )^\top \rvr \right]} \\
    &\le 1 + \sum\limits_{t=1}^{T-1}{ \left[ ( {\rvpi^*} - \rvpi_t )^\top \rvr + t^{ \beta - \frac{1}{3}} \right]} \\
    &\le 1 + \sum\limits_{t=1}^{T-1}{ \left( \frac{ K \ln{t} }{t} \right)^{\frac{1}{3}} } + \sum\limits_{t=1}^{T-1}{ ( {\rvpi^*} - \rvpi_t )^\top \rvr } \\
    &\le \underbrace{ 2 T^{\frac{2}{3}} \left( K \ln{T} \right)^{\frac{1}{3}} }_{\text{exploration cost}}+ \underbrace{ \sum\limits_{t=1}^{T-1}{ ( {\rvpi^*} - \rvpi_t )^\top \rvr }}_{\text{NN regret}},
\end{split}
\end{equation}
where the second inequality comes from $1 - t^{ \beta - \frac{1}{3}} \le 1$, $\forall t \ge 1$, and $\left\| \rvr \right\|_\infty \le 1$. The third inequality is according to the value of $\beta$. Next, we prove several events happen with high probabilities. Define
\begin{equation*}
    \gE_1 \triangleq \left\{ \| \hat{\rvr}_t - \rvr \|_\infty \le t^{\beta - \frac{1}{3}} \le ( K \ln{t} ) ^\frac{1}{3} t^{- \frac{1}{3}}, \quad \forall t \ge (K/3) \ln{\left(K/3\right) } \right\}.
\end{equation*}
According to \cref{thm:reward_estimation_hoeffding}, we have 
\begin{equation*}
    \probability\left\{ \gE_1 \right\} \ge
    1 - K \exp\left\{ - \frac{t^{\frac{1}{3}}}{2 K^2} \left( \frac{K}{3} \ln{t} \right)^{\frac{2}{3}} \right\} - 2 K t^{- \frac{1}{3}}.
\end{equation*}
Therefore the cumulative estimation error is upper bounded as,
\begin{equation}
\label{eq:estimation_upper_bound_appendix}
\begin{split}
    2 \sum\limits_{t=1}^{T-1}{ \| \rvr - \hat{\rvr}_t \|_\infty } &\le \frac{2 K}{3} \ln{\left(\frac{K}{3}\right) } + \underbrace{2 \sum\limits_{t=1}^{T-1}{ \left( K \ln{t} \right) ^\frac{1}{3} t^{- \frac{1}{3}} }}_{\gE_1 \text{ happens, } \probability\left\{ \gE_1 \right\} \le 1} + \underbrace{ 2  \sum\limits_{t=1}^{T-1}{ \left[ K \exp\left\{ - \frac{t^{\frac{1}{3}}}{2 K^2} \left( \frac{K}{3} \ln{t} \right)^{\frac{2}{3}} \right\} + 2 K t^{- \frac{1}{3}} \right] } }_{ \gE_1 \text{ does not happen, } \left\| \rvr - \hat{\rvr}_t \right\|_\infty \le 1 } \\
    &\le \frac{2 K}{3} \ln{\left(\frac{K}{3}\right) } + 3 T^{\frac{2}{3}} \left( K \ln{T} \right) ^\frac{1}{3} + 6 K T^{\frac{2}{3}} + o(1) \\
    &\le K \ln{K} + 3 T^{\frac{2}{3}} \left( K \ln{T} \right) ^\frac{1}{3} + 6 K T^{\frac{2}{3}}.
\end{split}
\end{equation}

Denote $\rvpi_t^* \coloneqq \argmax\limits_{\rvpi}{\left\{ \rvpi^\top \hat{\rvr}_t\right\}}$. The NN regret in \cref{eq:total_regret_decomposition_appendix} can be decomposed as,
\begin{equation}
\label{eq:playing_learning_phase_regret_decomposition_appendix}
\begin{split}
    \sum\limits_{t=1}^{T-1}{ ( {\rvpi^*} - \rvpi_t )^\top \rvr } &=  \sum\limits_{t=1}^{T-1}{ ( {\rvpi^*} - {\rvpi_t^*} )^\top \hat{\rvr}_t } +  \sum\limits_{t=1}^{T-1}{ ( {\rvpi_t^*} - \rvpi_t )^\top \hat{\rvr}_t } +  \sum\limits_{t=1}^{T-1}{ ( {\rvpi^*} - \rvpi_t )^\top ( \rvr - \hat{\rvr}_t ) } \\
    &\le  \sum\limits_{t=1}^{T-1}{ ( {\rvpi_t^*} - \rvpi_t )^\top \hat{\rvr}_t }  + \sum\limits_{t=1}^{T-1}{ \| \rvpi^* - \rvpi_t \|_1 \cdot \| \rvr - \hat{\rvr}_t \|_\infty } \\
    &\le  \sum\limits_{t=1}^{T-1}{ ( {\rvpi_t^*} - \rvpi_t )^\top \hat{\rvr}_t } + 2  \sum\limits_{t=1}^{T-1}{ \| \rvr - \hat{\rvr}_t \|_\infty } \\
    &\le \underbrace{ \sum\limits_{t=1}^{T-1}{ ( {\rvpi_t^*} - \rvpi_t )^\top \hat{\rvr}_t } }_{\text{dynamic NN regret}} + \underbrace{K \ln{K} + 3 T^{\frac{2}{3}} \left( K \ln{T} \right) ^\frac{1}{3} + 6 K T^{\frac{2}{3}}}_{\text{estimation error}},
\end{split}
\end{equation}
where the first inequality is by the definition of $\rvpi_t^*$ and  H{\"o}lder's inequality. The last inequality is from \cref{eq:estimation_upper_bound_appendix}. It remains to bound the dynamic NN regret in \cref{eq:playing_learning_phase_regret_decomposition_appendix}. Define
\begin{equation*}
    \gE_2 \triangleq \left\{ \sum\limits_{t=1}^{T-1}{ (  {\rvpi_t^*} - \rvpi_t )^\top \hat{\rvr}_t } \le \frac{7 K}{c} \cdot  T^{\frac{2}{3}}, \ c \in O(1) \right\}.
\end{equation*}

According to \cref{thm:dynamic_regret_sublinear}, we have,
\begin{equation*}
    \probability\left\{ \gE_2 \right\} \ge 1 - \exp\left\{ - \frac{T^2}{16 K^2} \right\} \ge 1 - \frac{16 K^2}{T^2}.
\end{equation*}
Therefore the dynamic NN regret in \cref{eq:playing_learning_phase_regret_decomposition_appendix} is upper bounded as,
\begin{equation}
\label{eq:dynamic_regret_upper_bound_appendix}
\begin{split}
    \sum\limits_{t=1}^{T-1}{ (  {\rvpi_t^*} - \rvpi_t )^\top \hat{\rvr}_t } &\le \underbrace{ (7K / c) \cdot  T^{\frac{2}{3}} }_{ \gE_2 \text{ happens}} + \underbrace{ (16 K / T^2) \cdot T }_{ \gE_2 \text{ does not happen}} \\
    &\le \frac{8 K}{c} \cdot  T^{\frac{2}{3}}.
\end{split}
\end{equation}

Finally, combining all the results \cref{eq:total_regret_decomposition_appendix}, \cref{eq:playing_learning_phase_regret_decomposition_appendix}, \cref{eq:estimation_upper_bound_appendix}, and \cref{eq:dynamic_regret_upper_bound_appendix}, we have,
\begin{equation*}
\begin{split}
    \sum\limits_{t=0}^{T-1}{ ( {\rvpi^*} - \tilde{\rvpi}_t )^\top \rvr} &\le \underbrace{ 2 T^{\frac{2}{3}} \left( K \ln{T} \right)^{\frac{1}{3}} }_{\text{ \cref{eq:total_regret_decomposition_appendix}}} + \underbrace{ K \ln{K} + 3 T^{\frac{2}{3}} \left( K \ln{T} \right) ^\frac{1}{3} + 6 K T^{\frac{2}{3}} }_{\text{\cref{eq:playing_learning_phase_regret_decomposition_appendix}, \cref{eq:estimation_upper_bound_appendix}}}+ \underbrace{ \frac{8 K}{c} \cdot T^{\frac{2}{3}} }_{\cref{eq:dynamic_regret_upper_bound_appendix}} \\
    &\le 5 T^{\frac{2}{3}} \left( K \ln{T} \right)^{\frac{1}{3}} + \frac{9 K}{c} \cdot  T^{\frac{2}{3}} + K \ln{K} \\
    &\le O(T^{\frac{2}{3}}( \ln{T} )^{\frac{1}{3}}). \qedhere
\end{split}
\end{equation*}
\end{proof}

\begin{thm}
\label{thm:reward_estimation_hoeffding}
    In \cref{alg:policy_gradient_uniform_exploration}, let $\beta = \frac{ \ln{(K/3) + \ln{\ln{t}} } }{ 3 \ln{t}}$. We have,
\begin{equation*}
    \| \hat{\rvr}_t - \rvr \|_\infty \le t^{\beta - \frac{1}{3}} \le ( K \ln{t} )^\frac{1}{3} t^{- \frac{1}{3}}, \ \forall t \ge (K/3) \ln{(K/3) },
\end{equation*}
with probability at least
\begin{equation*}
    1 - K \exp\left\{ - \frac{t^{\frac{1}{3} + 2 \beta}}{2 K^2} \right\} - 2 K \exp\left\{ - \frac{t^{3\beta}}{ K } \right\} = 1 - K \exp\left\{ - \frac{t^{\frac{1}{3}}}{2 K^2} \left( \frac{K}{3} \ln{t} \right)^{\frac{2}{3}} \right\} - 2 K t^{- \frac{1}{3}}.
\end{equation*}
\end{thm}
\begin{proof}
    Denote $\tilde{n}_t(k) \coloneqq \sum\limits_{s=0}^{t}{ \tilde{\pi}_t(k)}$, which can be lower bounded as,
\begin{equation*}
\begin{split}
    \tilde{n}_t(k) &\ge \sum\limits_{s=1}^{t}{ \tilde{\pi}_t(k)} = \sum\limits_{s=1}^{t}{ \left[ \left( 1 - s^{\beta - \frac{1}{3}} \right) \cdot \pi_{s}(k) + \frac{s^{\beta - \frac{1}{3}}}{K} \right] } \\
    &\ge \sum\limits_{s=1}^{t}{ \frac{s^{\beta - \frac{1}{3}}}{K} } \ge \frac{t^{\frac{2}{3} + \beta}}{ K  \left(\frac{2}{3} + \beta \right) } \ge \frac{t^{\frac{2}{3} + \beta}}{ K },
\end{split}
\end{equation*}
where the last inequality is by $\beta \le \frac{1}{3}$, $\forall t \ge (K/3) \ln{(K/3) }$. According to Theorem 2.3 in \citet{wainwright2015mathematical} or \citet{wainwright2019high},
\begin{equation*}
\begin{split}
    \probability\left\{ n_t(k) - \tilde{n}_t(k) < - a \right\} \le \exp\left\{ - \frac{a^2}{2 \sum_{s=1}^{t}{\sigma_s^2} } \right\} \le \exp\left\{ - \frac{2 a^2}{ t } \right\},
\end{split}
\end{equation*}
where $\sigma_s \le \frac{1}{4}$ is the variance of $\text{Bernoulli}( \tilde{\pi}_t(k))$. Therefore, the following event
\begin{equation*}
    \gE_3 \triangleq \left\{ n_t(k) \ge \tilde{n}_t(k) - \frac{t^{\frac{2}{3} + \beta}}{ 2 K } \ge \frac{t^{\frac{2}{3} + \beta}}{ 2 K}, \ \forall k \in [K] \right\},
\end{equation*}
holds with probability at least $1 - K \exp\left\{ -  \frac{t^{\frac{1}{3} + 2 \beta}}{2 K^2} \right\}$. Assume $\gE_3$ happens, according to the Hoeffding's inequality, the following event
\begin{equation*}
    \gE_4 \triangleq \left\{ | \hat{r}_{t}(k) - r(k) | \le t^{\beta - \frac{1}{3}} , \ \forall k \in [K] \right\}
\end{equation*}
holds with probability at least $1 - 2 K \exp\left\{ - \frac{t^{3\beta}}{ K } \right\}$.
\end{proof}

\begin{thm}
\label{thm:dynamic_regret_sublinear}
    If $m \ge T^2/K^2$, $\eta = \frac{1}{2 K m}$, then the dynamic NN regret satisfies,
\begin{equation*}
\begin{split}
    \sum\limits_{t=1}^{T-1}{ (  {\rvpi_t^*} - \rvpi_t )^\top \hat{\rvr}_t } \le \frac{7 K}{c} \cdot  T^{\frac{2}{3}},
\end{split}
\end{equation*}
with probability at least $1 - \exp\left\{ - \frac{T^2}{16 K} \right\} \ge 1 - \frac{16 K}{T^2}$.
\end{thm}
\begin{proof}
The proof bears similarity with \cref{thm:online_learning_regret}.
    Denote $\delta_t \coloneqq ( {\rvpi_t^*} - \rvpi_t )^\top \hat{\rvr}_t$. Decompose $\delta_t$ as,
\begin{equation}
\label{eq:dynamic_regret_decomposition_1}
\begin{split}
    \delta_t &= {\rvpi_t^*}^\top ( \hat{\rvr}_t - \hat{\rvr}_{t-1}) + {\rvpi_t^*}^\top \hat{\rvr}_{t-1} - \rvpi_t^\top \hat{\rvr}_t \\
    &\le \max\limits_{\rvpi}{ \left| \rvpi^\top ( \hat{\rvr}_t - \hat{\rvr}_{t-1}) \right| } + {\rvpi_t^*}^\top \hat{\rvr}_{t-1} - \rvpi_t^\top \hat{\rvr}_t \\
    &\le \left\| \hat{\rvr}_t - \hat{\rvr}_{t-1} \right\|_1 + {\rvpi_t^*}^\top \hat{\rvr}_{t-1} - \rvpi_t^\top \hat{\rvr}_t \\
    &= \frac{ \left| R_{t-1} - \hat{r}_{t-1}(A_{t-1}) \right| }{n_t(A_{t-1})} + {\rvpi_t^*}^\top \hat{\rvr}_{t-1} - \rvpi_t^\top \hat{\rvr}_t \\
    &\le \frac{ 1 }{n_t(A_{t-1})} + {\rvpi_t^*}^\top \hat{\rvr}_{t-1} - \rvpi_t^\top \hat{\rvr}_t \\
    &\le \frac{ 2 K }{t^{\frac{2}{3} + \beta}} + {\rvpi_t^*}^\top \hat{\rvr}_{t-1} - \rvpi_t^\top \hat{\rvr}_t,
\end{split}
\end{equation}
where the last inequality is by $n_{t}(A_{t-1}) \ge \frac{t^{\frac{2}{3} + \beta}}{ 2 K }$. Expanding \cref{eq:dynamic_regret_decomposition_1}, 
\begin{equation}
\label{eq:dynamic_regret_decomposition_2}
\begin{split}
    \delta_t &\le \frac{ 2 K }{t^{\frac{2}{3} + \beta}} + ( {\rvpi_t^*} - {\rvpi_{t-1}^*} )^\top \hat{\rvr}_{t-1} + {\rvpi_{t-1}^*}^\top \hat{\rvr}_{t-1} - \rvpi_t^\top \hat{\rvr}_t \\
    &\le \frac{ 2 K }{t^{\frac{2}{3} + \beta}} + {\rvpi_{t-1}^*}^\top \hat{\rvr}_{t-1} - \rvpi_t^\top \hat{\rvr}_t \\
    &= \frac{ 2 K }{t^{\frac{2}{3} + \beta}} + ( {\rvpi_{t-1}^*} - \rvpi_{t-1} )^\top \hat{\rvr}_{t-1} + \rvpi_{t-1}^\top \hat{\rvr}_{t-1} - \rvpi_t^\top \hat{\rvr}_t \\
    &= \frac{ 2 K }{t^{\frac{2}{3} + \beta}} + \delta_{t-1} + \rvpi_{t-1}^\top \hat{\rvr}_{t-1} - \rvpi_t^\top \hat{\rvr}_t \\  
    &= \frac{ 2 K }{t^{\frac{2}{3} + \beta}} + \delta_{t-1} + ( \rvpi_{t-1}  - \rvpi_t )^\top \hat{\rvr}_{t-1} + \rvpi_t^\top ( \hat{\rvr}_{t-1} - \hat{\rvr}_t) \\
    &\le \frac{ 2 K }{t^{\frac{2}{3} + \beta}} + \delta_{t-1} + ( \rvpi_{t-1}  - \rvpi_t )^\top \hat{\rvr}_{t-1} + \max\limits_{\rvpi}{ \left| \rvpi^\top ( \hat{\rvr}_t - \hat{\rvr}_{t-1}) \right| } \\
    &\le \frac{ 2 K }{t^{\frac{2}{3} + \beta}} + \delta_{t-1} + ( \rvpi_{t-1}  - \rvpi_t )^\top \hat{\rvr}_{t-1} + \frac{ 2 K }{t^{\frac{2}{3} + \beta}} \\
    &= \frac{ 4 K }{t^{\frac{2}{3} + \beta}} + \delta_{t-1} + ( \rvpi_{t-1}  - \rvpi_t )^\top \hat{\rvr}_{t-1}
\end{split}
\end{equation}
where the first inequality is by $( {\rvpi_t^*} - {\rvpi_{t-1}^*} )^\top \hat{\rvr}_{t-1} \le 0$. Upper bounding \cref{eq:dynamic_regret_decomposition_2},
\begin{equation}
\label{eq:dynamic_regret_decomposition_3}
\begin{split}
    \delta_t &\le \frac{ 4 K }{t^{\frac{2}{3} + \beta}} + \delta_{t-1} - \frac{1}{4 K m} \left\| \frac{d \rvpi_{t-1}^\top \hat{\rvr}_{t-1}}{d \rmW_{t-1}} \right\|_F^2  \qquad \left( \text{by \cref{lem:gradient_coupling}, \cref{lem:empirically_expected_reward_parameter_smoothness}}\right) \\
    &= \frac{ 4 K }{t^{\frac{2}{3} + \beta}} + \delta_{t-1} - \frac{1}{4 K m} \sum\limits_{r=1}^{m}{ \left\| \frac{d \rvpi_{t-1}^\top \hat{\rvr}_{t-1}}{d \rvw_r(t-1)} \right\|^2 } \\
    &\le \frac{ 4 K }{t^{\frac{2}{3} + \beta}} + \delta_{t-1} - \frac{c^2}{4 K} \left[ ( {\rvpi_{t-1}^*} - \rvpi_{t-1})^\top \hat{\rvr}_{t-1} \right]^2 \qquad \left( \text{by \cref{lem:gradient_lower_bound}}\right) \\
    &= \frac{ 4 K }{t^{\frac{2}{3} + \beta}} + \delta_{t-1} - \frac{c^2}{4 K} \cdot \delta_{t-1}^2.
\end{split}
\end{equation}
Rearranging and summing up from $1$ to $T-1$,
\begin{equation*}
\begin{split}
    \sum\limits_{t=1}^{T-1}{\delta_{t}^2} = \sum\limits_{t=2}^{T}{\delta_{t-1}^2} \le \frac{4 K}{ c^2} \cdot  \sum\limits_{t=2}^{T} { \left[ \delta_{t-1} - \delta_t + \frac{ 4 K }{t^{\frac{2}{3} + \beta}} \right] } \le \frac{49 K^2}{ c^2} \cdot T^{\frac{1}{3}}.
\end{split}
\end{equation*}
By the Root-Mean Square-Arithmetic Mean inequality,
\begin{equation*}
\begin{split}
    \sum\limits_{t=1}^{T-1}{\delta_{t}} \le \sqrt{\left(T  - 1 \right) \cdot \sum\limits_{t=1}^{T-1}{\delta_{t}^2}} \le \frac{7 K}{c} \cdot  T^{\frac{2}{3}}.
\end{split}
\end{equation*}
Let $\sigma = \frac{2 \sqrt{2}}{\sqrt{\pi m}}$ and $\tau = \frac{\sigma \sqrt{\pi}}{2 \sqrt{2}} = \frac{1}{\sqrt{m}}$ in \cref{lem:gradient_coupling}. Since $m \ge \frac{T^2}{K^2}$, we have $T \le \frac{\tau}{2 \eta}$, i.e., \cref{lem:gradient_coupling} holds within $T$ steps. By \cref{lem:gradient_coupling_in_total}, with probability at least $1 - \exp\left\{ - \frac{m}{8} ( 1 - \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma} ) \right\} = 1 - \exp\left\{ - \frac{m}{16} \right\} \ge 1 - \exp\left\{ - \frac{T^2}{16 K^2} \right\}$ over random initialization of neural networks, all the intermediate lemmas hold, therefore the results hold.
\end{proof}

\subsection{Proof for Lemmas Supporting \cref{thm:dynamic_regret_sublinear}}

\cref{thm:dynamic_regret_sublinear} relies on two results. First, expected reward is smooth in logit space, and small policy gradient updates preserve ReLU signs, therefore highly correlates logit derivative and policy gradient. Second, by the over-parameterization neural network theory, gradient norm is lower bounded by expected loss around initialization, which implies no bad local minima near the randomly initialized neural network policy $\rvpi_0$.

\begin{lem}
\label{lem:logit_smoothness}
Let $\rvpi(\rvo^\prime)$ and $\rvpi( \rvo)$ be the softmax policies of logit vectors $\rvo^\prime, \rvo \in \sR^K$, respectively. $\forall \rvr \in \left[ -1, 1\right]^K$,
\begin{equation*}
    ( \rvpi(\rvo^\prime) - \rvpi(\rvo)) ^\top \rvr \le \left\langle \frac{d \rvpi(\rvo)^\top \rvr}{d \rvo}, \rvo^\prime - \rvo \right\rangle + \| \rvo^\prime - \rvo \|^2.
\end{equation*}
\end{lem}
\begin{proof}
Denote $\rvo_{\xi} = \rvo + \xi ( \rvo^\prime - \rvo )$.
\begin{equation*}
\begin{split}
    &\left| \rvpi( \rvo^\prime )^\top \rvr - \rvpi( \rvo )^\top \rvr - \left\langle \frac{d \rvpi( \rvo )^\top \rvr}{d \rvo}, \rvo^\prime - \rvo \right\rangle \right| \\
    &\quad = \left| \int_0^1{ \frac{d \rvpi( \rvo + \xi ( \rvo^\prime - \rvo ) )^\top \rvr}{d \xi} d\xi} - \left\langle \frac{d \rvpi( \rvo )^\top \rvr}{d \rvo}, \rvo^\prime - \rvo \right\rangle \right| \\
    &\quad = \left| \int_0^1{ \left\langle \frac{d \rvpi( \rvo + \xi ( \rvo^\prime - \rvo ) )^\top \rvr}{d ( \rvo + \xi ( \rvo^\prime - \rvo ) )}, \rvo^\prime - \rvo \right\rangle d\xi} - \left\langle \frac{d \rvpi( \rvo )^\top \rvr}{d \rvo}, \rvo^\prime - \rvo \right\rangle \right| \\
    &\quad = \left| \int_0^1{ \left\langle \frac{d \rvpi( \rvo_{\xi} )^\top \rvr}{d \rvo_{\xi}} - \frac{d \rvpi( \rvo )^\top \rvr}{d \rvo}, \rvo^\prime - \rvo \right\rangle d\xi} \right| \\
    &\quad \le \int_0^1{ \left| \left\langle \frac{d \rvpi( \rvo_{\xi} )^\top \rvr}{d \rvo_{\xi}} - \frac{d \rvpi( \rvo )^\top \rvr}{d \rvo}, \rvo^\prime - \rvo \right\rangle \right| d\xi} \\
    &\quad \le \int_0^1{ \left\| \frac{d \rvpi( \rvo_{\xi} )^\top \rvr}{d \rvo_{\xi}} - \frac{d \rvpi( \rvo )^\top \rvr}{d \rvo} \right\| \cdot \| \rvo^\prime - \rvo \| d\xi} \\
    &\quad = \int_0^1{ \| \rmH( \rvpi( \rvo_{\xi} )) \rvr - \rmH( \rvpi( \rvo )) \rvr \| \cdot \| \rvo^\prime - \rvo \| d\xi} \\
    &\quad = \int_0^1{ \left\| \int_0^1{\left\langle \frac{d \rmH ( \rvpi( \rvo + \mu( \rvo_{\xi} - \rvo ) ) ) \rvr }{d ( \rvo + \mu( \rvo_{\xi} - \rvo ) )}, \rvo_{\xi} - \rvo \right\rangle d\mu} \right\| \cdot \| \rvo^\prime - \rvo \| d\xi} \\
    &\quad \le \int_0^1{  \int_0^1{ \left\| \left\langle \frac{d \rmH ( \rvpi( \rvo + \mu( \rvo_{\xi} - \rvo ) ) ) \rvr }{d ( \rvo + \mu( \rvo_{\xi} - \rvo ) )}, \rvo_{\xi} - \rvo \right\rangle \right\| d\mu} \cdot \| \rvo^\prime - \rvo \| d\xi} \\
    &\quad \le \int_0^1{  \int_0^1{ \left\| \frac{d \rmH ( \rvpi( \rvo + \mu( \rvo_{\xi} - \rvo ) ) ) \rvr }{d ( \rvo + \mu( \rvo_{\xi} - \rvo ) )} \right\| \cdot \| \rvo_{\xi} - \rvo \| d\mu} \cdot \| \rvo^\prime - \rvo \| d\xi} \\
    &\quad = \int_0^1{  \int_0^1{ \left\| \frac{d \rmH ( \rvpi( \rvo + \mu( \rvo_{\xi} - \rvo ) ) ) \rvr }{d ( \rvo + \mu( \rvo_{\xi} - \rvo ) )} \right\| d\mu} \cdot \xi \cdot \| \rvo^\prime - \rvo \|^2 d\xi} \\
    &\quad \le \int_0^1{  \int_0^1{ 2 d\mu} \cdot \xi \cdot \| \rvo^\prime - \rvo \|^2 d\xi} \\
    &\quad = \| \rvo^\prime - \rvo \|^2,
\end{split}
\end{equation*}
where the last inequality is according to \cref{lem:Hr_spectral_norm_upper_bound} (also for the definition of $\rmH$).
\end{proof}

\begin{lem}
\label{lem:Hr_spectral_norm_upper_bound}
    Let $\rvo \in \sR^K$ be any logit vector, and $\rvpi(\rvo)$ be the softmax policy of $\rvo$. Denote $\rmH( \rvpi (\rvo ) ) \coloneqq \Delta ( \rvpi (\rvo ) ) - \rvpi (\rvo ) \rvpi (\rvo )^\top$, where $\Delta ( \rvpi (\rvo ) ) \in \sR^{K \times K}$ is a diagonal matrix with $\rvpi (\rvo )$ as its diagonal. $\forall \rvr \in \left[ -1, 1\right]^K$,
\begin{equation*}
    \left\| \frac{d \rmH( \rvpi (\rvo ) ) \rvr}{d \rvo } \right\| \le 2,
\end{equation*}
where $\| \cdot \|_2$ is the matrix spectral norm.
\end{lem}
\begin{proof}
    Denote $\rmS \coloneqq \frac{d \rmH( \rvpi (\rvo ) ) \rvr}{d \rvo } \in \sR^{K \times K}$. $\forall s, t \in [K]$,
\begin{equation*}
\begin{split}
    S_{s, t} &= \frac{d \pi_{s} ( r_{s} - \rvpi^\top \rvr ) }{d o_{t}} \\
    &= \frac{d \pi_{s} }{d o_{t}} ( r_{s} - \rvpi^\top \rvr ) + \pi_{s} \frac{d ( r_{s} - \rvpi^\top \rvr ) }{d o_{t}} \\
    &= ( \sI\{ s = t\} \pi_{t} -  \pi_{s } \pi_{t} ) ( r_{s} - \rvpi^\top \rvr ) - \pi_{s} ( \pi_{t} r_{t} - \pi_{t} \rvpi^\top \rvr ).
\end{split}
\end{equation*}
For any $\rvx \in \sR^K$, 
\begin{equation*}
\begin{split}
    \rvx^\top \rmS \rvx &= \sum\limits_{s=1}^{h}{ \sum\limits_{t=1}^{h}{ S_{s,t} x_s x_t} } \\
    &= \expectation\limits_{\rvpi}\left[ \rvr \cdot \rvx \cdot \rvx \right] - \expectation\limits_{\rvpi}\left[ \rvr \right] \cdot \expectation\limits_{\rvpi}\left[ \rvx \cdot \rvx \right] - \expectation\limits_{\rvpi}\left[ \rvr \cdot \rvx \right] \cdot \expectation\limits_{\rvpi}\left[ \rvx \right] + \expectation\limits_{\rvpi}\left[ \rvr \right] \cdot \left( \expectation\limits_{\rvpi}\left[ \rvx \right] \right)^2  - \expectation\limits_{\rvpi}\left[ \rvr \cdot \rvx \right] \cdot \expectation\limits_{\rvpi}\left[ \rvx \right] + \expectation\limits_{\rvpi}\left[ \rvr \right] \cdot \left( \expectation\limits_{\rvpi}\left[ \rvx \right] \right)^2 \\
    &\le \expectation\limits_{\rvpi}\left[ \rvr \cdot \rvx \cdot \rvx \right] - \expectation\limits_{\rvpi}\left[ \rvr \cdot \rvx \right] \cdot \expectation\limits_{\rvpi}\left[ \rvx \right] - \expectation\limits_{\rvpi}\left[ \rvr \cdot \rvx \right] \cdot \expectation\limits_{\rvpi}\left[ \rvx \right] + \expectation\limits_{\rvpi}\left[ \rvr \right] \cdot \left( \expectation\limits_{\rvpi}\left[ \rvx \right] \right)^2 \\
    &\le \| \rvpi \| \cdot \| \rvr \cdot \rvx \cdot \rvx \| + 2 \cdot \| \rvpi \| \cdot \| \rvr \cdot \rvx \| \cdot \| \rvpi \| \cdot \| \rvx \| + \| \rvpi \|^2 \cdot \| \rvr \cdot \rvx \|^2 \\
    &\le \| \rvr \cdot \rvx \cdot \rvx \| + 2 \cdot \| \rvr \cdot \rvx \| \cdot \| \rvx \| + \| \rvr \cdot \rvx \|^2 \\
    &\le \| \rvx \cdot \rvx \| + 2 \cdot \| \rvx \|^2 + \| \rvx \|^2 \\
    &\le 4 \cdot \| \rvx \|^2. \qedhere
\end{split}
\end{equation*}
\end{proof}

\begin{lem}
\label{lem:gradient_coupling}
	Define the pseudo policy gradient at step $t$ as,
\begin{equation*}
\begin{split}
	\frac{d \tilde{\ell}_t}{d \rmW_t} \coloneqq \tilde{\rmD} \rmA^\top \rmH( \rvpi_t ) \hat{\rvr}_t \rvs^\top,
\end{split}
\end{equation*}
where $\tilde{\rmD} \in \sR^{K \times K}$ is a diagonal matrix, and  $\tilde{\rmD}_{r,r} \triangleq \sI\{ \rvw_r(0)^\top \rvs > 0 \}$, $\forall r \in [m]$. $\rmH( \rvpi_t )$ is,
\begin{equation*}
    \rmH( \rvpi_t ) \coloneqq \Delta( \rvpi_t ) - \rvpi_t \rvpi_t^\top.
\end{equation*}
The true policy gradient is,
\begin{equation*}
\begin{split}
    \frac{d \rvpi_t^\top \hat{\rvr}_t}{d \rmW_t} \coloneqq  \rmD(t) \rmA^\top \rmH( \rvpi_t ) \hat{\rvr}_t \rvs^\top.
\end{split}
\end{equation*}
where $\rmD_{r,r}(t) \coloneqq \sI\{ \rvw_r(t)^\top \rvs > 0 \}$, $\forall r \in [m]$. $\forall \tau > 0$, $\forall r \in [m]$, with probability at least $1 - \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma}$, $\forall t \le \frac{\tau}{ 2 \eta }$,
\begin{equation*}
	\frac{d\tilde{\ell}_t}{d \rvw_r(t)} = \frac{d \rvpi_t^\top \hat{\rvr}_t}{d \rvw_r(t)},
\end{equation*}
where $\rvw_r(t)$ is the $r$th row vector of $\rmW_t$.
\end{lem}
\begin{proof}
The $r$th row of the pseudo policy gradient as,
\begin{equation*}
	\frac{d \tilde{\ell}_t}{d \rvw_r(t)} \coloneqq \sum\limits_{k=1}^{K}{ \left[ \hat{r}_{t}(k) \cdot \pi_{t}(k) \cdot \left( \sum\limits_{k^\prime \not= k}^{K}{ \pi_{t}\left(k^\prime\right) \cdot \left( a_{k,r} - a_{k^\prime,r} \right)  } \right) \cdot \sI\{ \rvw_r(0)^\top \rvs > 0 \} \cdot \rvs \right] },
\end{equation*}
While the $r$th row of the true policy gradient is,
\begin{equation*}
	\frac{d \rvpi_t^\top \hat{\rvr}_t}{d \rvw_r(t)} = \sum\limits_{k=1}^{K}{ \left[\hat{r}_{t}(k) \cdot \pi_{t}(k) \cdot \left( \sum\limits_{k^\prime \not= k}^{K}{ \pi_{t}\left(k^\prime\right) \cdot \left( a_{k,r} - a_{k^\prime,r} \right)  } \right) \cdot \sI\{ \rvw_r(t)^\top \rvs > 0 \} \cdot \rvs \right] }.
\end{equation*}
The true policy gradient norm is upper bounded by the empirically expected reward,
\begin{equation*}
\begin{split}
	\left\| \frac{d \rvpi_t^\top \hat{\rvr}_t}{d \rvw_r(t)} \right\|_2 &\le \sum\limits_{k=1}^{K}{ \left|\hat{r}_{t}(k) \cdot \pi_{t}(k) \cdot \sum\limits_{k^\prime \not= k}^{K}{ \pi_{t}(k^\prime) \cdot ( a_{k,r} - a_{k^\prime,r} )  } \cdot \sI\{ \rvw_r(t)^\top \rvs > 0 \} \right| \cdot \| \rvs \| } \\
	&\le 2 \cdot \sum\limits_{k=1}^{K}{\hat{r}_{t}(k) \cdot \pi_{t}(k) \cdot \sum\limits_{k^\prime \not= k}^{K}{ \pi_{t}(k^\prime)  } \cdot \| \rvs \|  } \\
	&\le 2 \cdot \rvpi_t^\top \hat{\rvr}_t.
\end{split}
\end{equation*}
After $t$ times of gradient updates, the distance between $\rvw_r(t)$ and $\rvw_r(0)$ is also upper bounded,
\begin{equation*}
\begin{split}
	\| \rvw_r(t) - \rvw_r(0) \| &\le \eta \cdot \sum\limits_{s=0}^{t-1}{\left\| \frac{d \rvpi_s^\top \hat{\rvr}_s}{d \rvw_r(s)} \right\|} \\
	&\le 2 \eta \cdot \sum\limits_{s=0}^{t-1}{ \rvpi_s^\top \hat{\rvr}_s } \\
	&\le 2 \eta t .
\end{split}
\end{equation*}
Since $\rvw_r(0)^\top \rvs \sim \gN(0, \sigma^2)$, $\probability\left\{ | \rvw_r(0)^\top \rvs | \le \tau\right\} \le \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma}$,
\begin{equation*}
\begin{split}
	\probability\left\{ | \rvw_r(0)^\top \rvs | > \tau\right\} &= 1 - \probability\left\{ | \rvw_r(0)^\top \rvs | \le \tau\right\} \\
	&\ge 1 - \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma}.
\end{split}
\end{equation*}
Conditioning on the above event happens, i.e., $| \rvw_r(0)^\top \rvs | > \tau$, let $t \le \frac{\tau}{ 2 \eta }$,
\begin{equation*}
\begin{split}
	| ( \rvw_r(t) - \rvw_r(0) )^\top \rvs | &\le \| \rvw_r(t) - \rvw_r(0) \| \cdot \| \rvs \| \\
	&\le 2 \eta t \\
	&\le \tau < | \rvw_r(0)^\top \rvs |,
\end{split}
\end{equation*}
which implies if $| \rvw_r(0)^\top \rvs | > \tau$, then,
\begin{equation*}
\begin{split}
	\sI\{ \rvw_r(t)^\top \rvs > 0 \} &= \sI\{ \rvw_r(0)^\top \rvs  + ( \rvw_r(t) - \rvw_r(0) )^\top \rvs > 0 \} \\
	&= \sI\{ \rvw_r(0)^\top \rvs > 0 \}. \qedhere
\end{split}
\end{equation*}
\end{proof}

=======================updated

\cref{lem:gradient_coupling} implies that for bounded numbers of policy gradient updates, the signs of the ReLUs will not change, i.e., $\sI\left\{ \rvw_r(t)^\top \rvs > 0 \right\} = \sI\left\{ \rvw_r(0)^\top \rvs > 0 \right\}$. Combine \cref{lem:logit_smoothness} with \cref{lem:gradient_coupling}, we have the smoothness property of the surrogate expected loss in the parameter space.
\begin{lem}
\label{lem:empirically_expected_reward_parameter_smoothness}
    $\rmW_{t+1} = \rmW_t + \eta \cdot \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t}$. $\forall t \le \frac{\tau}{ 2 \eta }$,
\begin{equation}
\label{eq:parameter_smoothness}
\begin{split}
    \rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t - \rvpi\left( \rmW_{t+1} \right)^\top \hat{\rvr}_t \le - \left( \eta - h m \eta^2 \right) \cdot \left\| \frac{d \rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t}{d \rmW_t} \right\|_F^2.
\end{split}
\end{equation}
\end{lem}
\begin{proof}
   Let $\rvr = - \hat{\rvr}_t$ in \cref{lem:logit_smoothness}. By \cref{lem:inner_product_logit_difference_logit_derivative} and \cref{lem:logit_upper_bound_parameter},
\begin{equation*}
\begin{split}
    \rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t - \rvpi\left( \rmW_{t+1} \right)^\top \hat{\rvr}_t &= \rvpi\left( \rmW_{t+1} \right)^\top \left[ - \hat{\rvr}_t \right] - \rvpi\left( \rmW_t \right)^\top \left[ - \hat{\rvr}_t \right]  \\
    &= \rvpi\left( \rvo_{t+1} \right)^\top \left[ - \hat{\rvr}_t \right] - \rvpi\left( \rvo_t \right)^\top \left[ - \hat{\rvr}_t \right] \\
    &\le \left\langle - \frac{d \rvpi\left( \rvo_t \right)^\top \hat{\rvr}_t}{d \rvo_t}, \rvo_{t+1} - \rvo_t \right\rangle + \left\| \rvo_{t+1} - \rvo_t  \right\|_2^2 \\
    &\le - \eta \cdot \left\| \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t} \right\|_F^2 + h m \left\| \rmW_{t+1} - \rmW_t \right\|_F^2 \\
    &= - \eta \cdot \left\| \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t} \right\|_F^2 + h m \eta^2 \left\| \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t} \right\|_F^2. \qedhere
\end{split}
\end{equation*}
\end{proof}

\begin{lem}
\label{lem:inner_product_logit_difference_logit_derivative}
    $\rmW_{t+1} = \rmW_t + \eta \cdot \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t}$. Denote $\rvo_{t+1}$ and $\rvo_t$ as the logit vectors of $\rmW_t$ and $\rmW_{t+1}$ at state $\rvs$, respectively. $\forall t \le \frac{\tau}{ 2 \eta }$,
\begin{equation*}
\begin{split}
    \left\langle - \frac{d \rvpi\left( \rvo_t \right)^\top \hat{\rvr}_t}{d \rvo_t}, \rvo_{t+1} - \rvo_t \right\rangle = - \eta \left\| \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t} \right\|_F^2.
\end{split}
\end{equation*}
\end{lem}
\begin{proof}
    First, the negative logit derivative is,
\begin{equation*}
\begin{split}
    - \frac{d \rvpi\left( \rvo_t \right)^\top \hat{\rvr}_t}{d \rvo_t} = \left[\Delta\left( \rvpi\left( \rvo_t \right) \right) - \rvpi\left( \rvo_t \right) \rvpi\left( \rvo_t \right)^\top \right] \hat{\rvr}_t = \left[\Delta\left( \rvpi\left( \rmW_t \right) \right) - \rvpi\left( \rmW_t \right) \rvpi\left( \rmW_t \right)^\top \right] \hat{\rvr}_t.
\end{split}
\end{equation*}
Second, the logit difference of policy gradient update is,
\begin{equation*}
\begin{split}
    \rvo_{t+1} - \rvo_t = \rmA \left[ \sigma\left(\rmW_{t+1} \rvs \right) - \sigma\left( \rmW_t \rvs \right)\right] = \rmA \rmD(t) \left[ \rmW_{t+1} - \rmW_t \right] \rvs = \eta \cdot \rmA \rmD(t) \left[ \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t} \right] \rvs.
\end{split}
\end{equation*}
Combining the two above results,
\begin{equation*}
\begin{split}
    \left\langle - \frac{d \rvpi\left( \rvo_t \right)^\top \hat{\rvr}_t}{d \rvo_t}, \rvo_{t+1} - \rvo_t \right\rangle &= - \eta \cdot \hat{\rvr}_t^\top \left[\Delta\left( \rvpi\left( \rmW_t \right) \right) - \rvpi\left( \rmW_t \right) \rvpi\left( \rmW_t \right)^\top \right] \rmA \rmD(t) \left[ \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t} \right] \rvs \\
    &= - \eta \cdot \trace \left\{ \hat{\rvr}_t^\top \left[\Delta\left( \rvpi\left( \rmW_t \right) \right) - \rvpi\left( \rmW_t \right) \rvpi\left( \rmW_t \right)^\top \right] \rmA \rmD(t) \left[ \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t} \right] \rvs \right\} \\
    &= - \eta \cdot \trace \left\{ \rvs \hat{\rvr}_t^\top \left[\Delta\left( \rvpi\left( \rmW_t \right) \right) - \rvpi\left( \rmW_t \right) \rvpi\left( \rmW_t \right)^\top \right] \rmA \rmD(t) \left[ \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t} \right]  \right\} \\
    &= - \eta \left\langle \rmD(t) \rmA^\top \left[\Delta\left( \rvpi\left( \rmW_t \right) \right) - \rvpi\left( \rmW_t \right) \rvpi\left( \rmW_t \right)^\top \right] \hat{\rvr}_t \rvs^\top, \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t} \right\rangle \\
    &= - \eta \cdot \left\| \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rmW_t} \right\|_F^2. \qedhere
\end{split}
\end{equation*}
\end{proof}

\begin{lem}
\label{lem:logit_upper_bound_parameter}
Let $\rvo$ and $\rvo^\prime$ be the logit vectors of  $\rmW$ and $\rmW^\prime$, respectively, i.e.,
\begin{equation*}
\begin{split}
    &o_{k} = \sum\limits_{r=1}^{m}{ a_{k,r} \cdot \sigma\left( u_{r} \right)} = \sum\limits_{r=1}^{m}{ a_{k,r} \cdot \sigma\left(\rvw_r^\top \rvs \right)} \\
    &o_{k}^\prime = \sum\limits_{r=1}^{m}{ a_{k,r} \cdot \sigma\left( u_{r}^\prime \right)} = \sum\limits_{r=1}^{m}{ a_{k,r} \cdot \sigma\left({\rvw_r^\prime}^\top \rvs \right)},
\end{split}
\end{equation*}
$\forall k \in [h]$. Then,
\begin{equation*}
\begin{split}
    \left\| \rvo^\prime - \rvo \right\|_2^2 \le h m \left\| \rmW^\prime - \rmW \right\|_F^2.
\end{split}
\end{equation*}
\end{lem}
\begin{proof}
Denote $k_* \triangleq \argmax\limits_{k \in [h]}\left\{ \left| o_{k}^\prime - o_{k} \right| \right\}$. By the $1$-Lipschitzness of ReLU,
\begin{equation*}
\begin{split}
    \left\| \rvo^\prime - \rvo \right\|_2^2 &\le \sum\limits_{k = 1}^{h}{ \left\| \rvo^\prime - \rvo \right\|_\infty^2} = h \cdot \left| o_{k_*}^\prime - o_{k_*} \right|^2 \\
    &= h \cdot \left| \sum\limits_{r=1}^{m}{ a_{k_*,r} \cdot \left( \sigma\left({\rvw_r^\prime}^\top \rvs \right) - \sigma\left(\rvw_r^\top \rvs \right) \right)} \right|^2 \\
    &\le h \cdot \left( \sum\limits_{r=1}^{m}{ \left| a_{k_*,r} \right| \cdot \left| \sigma\left({\rvw_r^\prime}^\top \rvs\right) - \sigma\left({\rvw_r}^\top \rvs\right) \right|  } \right)^2 \\
    &\le h \cdot \left( \sum\limits_{r=1}^{m}{ \left| \left({\rvw_r^\prime} - \rvw_r \right)^\top \rvs\right|  } \right)^2 \\
    &\le h \cdot \left( \sum\limits_{r=1}^{m}{ \left\| {\rvw_r^\prime} - \rvw_r \right\|_2  } \right)^2 \\
    &\le h \cdot \left( \sqrt{m} \cdot \left\| \rmW^\prime - \rmW \right\|_F \right)^2 \\
    &= h m \cdot \left\| \rmW^\prime - \rmW \right\|_F^2. \qedhere
\end{split}
\end{equation*}
\end{proof}

Now by the key insight of the recent progresses of the over-parameterized neural network optimization theory, with constants probability, the pseudo gradient norm is lower bounded by the objective \citep{li2018learning}.

\begin{lem}
\label{lem:gradient_lower_bound}
	Denote $\hat{k}_t^* \triangleq \argmax\limits_{k \in [h]}\left\{ \hat{r}_{t}(k) \right\}$, i.e., the optimal action using the estimated reward $ \hat{\rvr}_t$. If $\pi_{t}(\hat{k}_t^*) > c > 0$, with probability $\frac{3}{64} \in \Omega\left( 1 \right)$,
\begin{equation*}
\begin{split}
	\left\| \frac{d\tilde{\ell}_t}{d \rvw_r(t)} \right\|_2 \ge c \cdot \left( \max\limits_{k \in \left[h\right]}\left\{ \hat{r}_{t}(k) \right\} - \rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t \right) .
\end{split}
\end{equation*}
\end{lem}


\begin{proof}
	 For conciseness, denote $\hat{k}^*(t)$ as $\hat{k}^*$. Rewrite $\frac{d\tilde{\ell}_{t}}{d \rvw_r(t)} = \sum\limits_{k^
	\prime=1}^{h}{ a_{k^\prime,r} \cdot \rvp_{k^\prime, r} }$, where $\rvp_{k^\prime, r} \in \sR^d$ is defined as, 
\begin{equation*}
	\rvp_{k^\prime, r} \triangleq \sum\limits_{k=1}^{h}{ \left[ \hat{r}_{t}(k) \cdot \pi_{t}(k) \cdot v_{k^\prime,k}(t) \cdot \sI\left\{ \rvw_r(0)^\top \rvs > 0 \right\} \cdot \rvs \right] },
\end{equation*}
and $v_{k^\prime,k}(t)$ is defined as,
\begin{equation*}
	v_{k^\prime,k}(t) = \begin{cases}
    1 - \pi_{t}\left(k^\prime\right), & \text{if $k^\prime = k$}, \\
    - \pi_{t}\left(k^\prime\right), & \text{otherwise}.
  \end{cases}
\end{equation*}
By the randomness of $a_{k^\prime,r}$,
\begin{equation}
\label{eq:gradient_p_lowerbound}
\begin{split}
	\left\| \frac{d\tilde{\ell}_t}{d \rvw_r(t)} \right\|_2 = \left\| \sum\limits_{k^
	\prime=1}^{h}{ a_{k^\prime,r} \cdot \rvp_{k^\prime, r} } \right\|_2 \ge \left| a_{\hat{k}^*,r} \right| \cdot \left\| \rvp_{\hat{k}^*, r}\right\|_2 = \left\| \rvp_{\hat{k}^*, r}\right\|_2,
\end{split}
\end{equation}
with probability $\frac{1}{2}$. Define $h_{\hat{k}^*,r}$ as follows,
\begin{equation}
\label{eq:h_auxiliary}
\begin{split}
	h_{\hat{k}^*,r} \triangleq \rvw_r(0)^\top \rvp_{\hat{k}^*, r} =  \sum\limits_{k=1}^{h}{ \left[ \hat{r}_{t}(k) \cdot \pi_{t}(k) \cdot v_{\hat{k}^*,k}(t) \cdot \sigma( \rvw_r(0)^\top \rvs ) \right] }.
\end{split}
\end{equation}
Denote $x \triangleq \rvw_r(0)^\top \rvs \sim \gN(0, \sigma^2)$. Note that $h_{\hat{k}^*,r}$ is a convex function of $x$. Moreover,
\begin{equation*}
\begin{split}
	\sum\limits_{k=1}^{h}{ \left[ \hat{r}_{t}(k) \cdot \pi_{t}(k) \cdot v_{\hat{k}^*,k}(t) \right] } &= \hat{r}_{t}(\hat{k}^*) \cdot \pi_{t}(\hat{k}^*) \cdot v_{\hat{k}^*,\hat{k}^*}(t) + \sum\limits_{k\not=\hat{k}^*}{ \left[ \hat{r}_{t}(k) \cdot \pi_{t}(k) \cdot v_{\hat{k}^*,k}(t) \right] } \\
	&= \hat{r}_{t}(\hat{k}^*) \cdot \pi_{t}(\hat{k}^*) \cdot \left( 1 - \pi_{t}(\hat{k}^*) \right) - \pi_{t}(\hat{k}^*) \cdot \sum\limits_{k\not=\hat{k}^*}{ \left[ \hat{r}_{t}(k) \cdot \pi_{t}(k) \right] } \\
	&= \pi_{t}(\hat{k}^*) \cdot \left( \hat{r}_{t}(\hat{k}^*) - \sum\limits_{k=1}^{h}{ \left[ \hat{r}_{t}(k) \cdot \pi_{t}(k) \right] } \right) \\
	&= \pi_{t}(\hat{k}^*) \cdot \left( \max\limits_{k \in \left[h\right]}\left\{ \hat{r}_t(k) \right\} - \rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t  \right).
\end{split}
\end{equation*}
By assumption, if $\pi_{t}(\hat{k}^*) > c > 0$, then ,
\begin{equation*}
\begin{split}
	\left| \partial{h_{\hat{k}^*,r}}_{\max}{(0)} - \partial{h_{\hat{k}^*,r}}_{\min}{(0)} \right| &= \left| \sum\limits_{k=1}^{h}{ \left[  \hat{r}_{t}(k) \cdot \pi_{t}(k) \cdot v_{\hat{k}^*,k}(t) \right] } \right| \\
	&= \pi_{t}(\hat{k}^*) \cdot \left( \max\limits_{k \in \left[h\right]}\left\{ \hat{r}_t(k) \right\} - \rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t  \right) \\
	&\ge c \cdot \left( \max\limits_{k \in \left[h\right]}\left\{ \hat{r}_t(k) \right\} - \rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t  \right).
\end{split}
\end{equation*}
where $\partial{h_{\hat{k}^*,r}}_{\max}{(0)} = \max\left\{ \partial{h_{\hat{k}^*,r}}{(0)} \right\}$, $\partial{h_{\hat{k}^*,r}}_{\min}{(0)} = \min\left\{ \partial{h_{\hat{k}^*,r}}{(0)} \right\}$ are the maximum and minimum of $\partial{h_{\hat{k}^*,r}}{(0)}$, i.e., the subdifferential of $h_{\hat{k}^*,r}$ at $0$. Now consider \cref{eq:h_auxiliary}, and uniformly randomly generate $x$ in $\left[ -\tau, \tau \right]$, $\forall \tau > 0$, by \cref{lem:non_smooth_convex},
\begin{equation}
\label{eq:h_regret_lower_bound}
\begin{split}
	\probability\limits_{x \sim  \gU[-\tau, \tau]}\left\{ \left|  h_{\hat{k}^*,r} \right| \ge \frac{c\tau}{8} \cdot \left( \max\limits_{k \in \left[h\right]}\left\{ \hat{r}_t(k) \right\} - \rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t  \right) \right\} > \frac{1}{8}.
\end{split}
\end{equation}
Finally, $h_{\hat{k}^*,r} \sim \gN\left( 0, \left\| \rvp_{\hat{k}^*, r} \right\|_2^2 \cdot \sigma^2 \right)$, with probability at least $\frac{3}{4}$,
\begin{equation}
\label{eq:p_h_lower_bound}
\begin{split}
	\left| h_{\hat{k}^*,r} \right| < \sqrt{2 \ln{8}} \cdot \left\| \rvp_{\hat{k}^*, r} \right\|_2 \cdot \sigma.
\end{split}
\end{equation}
Combining \cref{eq:gradient_p_lowerbound}, \cref{eq:h_regret_lower_bound} and \cref{eq:p_h_lower_bound}, we have,
\begin{equation*}
\begin{split}
	\left\| \frac{d\tilde{\ell}_t}{d \rvw_r(t)} \right\|_2 \ge \left\| \rvp_{\hat{k}^*, r}\right\|_2 > \frac{\left| h_{\hat{k}^*,r} \right|}{\sqrt{2 \ln{8}} \sigma} \ge \frac{c\tau}{8 \sqrt{2 \ln{8}} \sigma} \cdot \left( \max\limits_{k \in \left[h\right]}\left\{ \hat{r}_t(k) \right\} - \rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t  \right),
\end{split}
\end{equation*}
with probability at least $\frac{1}{2}  \cdot \frac{1}{8} \cdot \frac{3}{4} $. Taking $\tau = 8 \sqrt{2 \ln{8}} \sigma$ completes the proof.
\end{proof}

\begin{lem}
\label{lem:non_smooth_convex}
	Let $\phi(x) : \sR \to \sR$ be a convex function non-smooth at $0$. Define,
\begin{equation*}
\begin{split}
	\partial\phi_{\max}{(0)} = \max\left\{ \partial\phi(0) \right\}, \quad \partial\phi_{\min}{(0)} = \min\left\{ \partial\phi(0) \right\},
\end{split}
\end{equation*}	
where $\partial\phi(0)$ is the subdifferential of $\phi$ at $0$. Then we have,
\begin{equation*}
\begin{split}
	\probability\limits_{x \sim \gU[-\tau, \tau]}\left\{ \left| \phi(x) \right| \ge \frac{ \left( \partial\phi_{\max}{(0)} - \partial\phi_{\min}{(0)} \right) \tau}{8} \right\} \ge \frac{1}{8}.
\end{split}
\end{equation*}	
\end{lem}
\begin{proof}
	Denote $\rho = \partial\phi_{\max}{(0)} - \partial\phi_{\min}{(0)}$, we have $\partial\phi_{\max}{(0)} \ge \frac{\rho}{2}$ or $\partial\phi_{\min}{(0)} \le - \frac{\rho}{2}$. If $\partial\phi_{\min}{(0)} \le - \frac{\rho}{2}$, then $-\phi$ will satisfy the first case. So we prove for $\partial\phi_{\max}{(0)} \ge \frac{\rho}{2}$. Define $\hat{\phi}(x) = \phi(x) - \phi(0)$. $\forall x > 0$,
\begin{equation*}
\begin{split}
	\hat{\phi}(x) \ge \hat{\phi}(0) + \frac{\rho}{2} \cdot \left( x - 0\right) = \frac{\rho x}{2} \ge 0.
\end{split}
\end{equation*}
If $\phi(0) \ge 0$, then $\forall x \in \left[\frac{\tau}{2}, \tau \right]$,
\begin{equation*}
\begin{split}
	\left| \phi(x) \right| = \left| \hat{\phi}(x) + \phi(0) \right| = \hat{\phi}(x) + \phi(0) \ge \frac{\rho x}{2} \ge \frac{\rho \tau}{4}.
\end{split}
\end{equation*}
If $\phi(0) < 0$, then $\phi(x_0) = 0$ for some $x_0 > 0$. If $x_0 < \frac{\tau}{2}$, $\forall x \in \left[x_0 + \frac{\tau}{4}, \tau \right]$,
\begin{equation*}
\begin{split}
	\left| \phi(x) \right| =  \phi(x) \ge \phi(x_0) + \phi^\prime(x_0) \cdot \left( x - x_0 \right) \ge \frac{\rho \tau}{8}.
\end{split}
\end{equation*}
If $x_0 \ge \frac{\tau}{2}$, then $\phi(0) \le - \frac{\rho}{2} \cdot x_0 \le - \frac{\rho\tau}{4}$, $\forall x \in \left[0, x_0 - \frac{\tau}{4} \right]$,
\begin{equation*}
	\left| \phi(x) \right| \ge \left| \frac{-\phi(0)}{x_0} \left( x - x_0 \right) \right| = \frac{-\phi(0)}{x_0} \left( x_0 - x \right) \ge \frac{\rho\tau}{8}. \qedhere
\end{equation*}
\end{proof}

\cref{lem:gradient_lower_bound} generalizes the over-parameterized neural network optimization theory into the RL settings. By \cref{lem:gradient_lower_bound}, whenever the policy empirically expected reward $\rvpi\left( \rmW_t \right)^\top \hat{\rvr}_t$ small comparing with the largest possible empirical reward $\max\limits_{k \in \left[h\right]}\left\{ \hat{r}_{t}(k) \right\}$, with enough exploration of the suggorate optimal action ($\pi_{t}(\hat{k}_t^*) > c > 0$), with constant probability, the pseudo policy gradient norm will also be large. Therefore, combining \cref{lem:gradient_lower_bound} with \cref{lem:gradient_coupling}, the true policy gradient norm is also large, which is necessary for using \cref{lem:empirically_expected_reward_parameter_smoothness}. Applying all the stated lemmas, the policy surrogate expected loss converges as shown in \cref{thm:dynamic_regret_sublinear}.

\begin{lem}[Chernoff]
\label{lem:chernoff}
    Let $X_1, X_2, \dots, X_n \sim B(1, p)$ be independent Bernoulli random variables. Define $X = \sum\limits_{i=1}^{n}{ X_i  }$. Denote $\mu \triangleq \sE\left[ X \right]$. $\forall \alpha \in [0,1]$,
\begin{equation*}
    \probability\left\{ X \le (1 - \alpha) \mu \right\} \le \exp\left\{ - \frac{\alpha^2 \mu}{2} \right\}.
\end{equation*}
\end{lem}

\begin{lem}
\label{lem:gradient_coupling_in_total}
With probability at least $1 - \exp\left\{ - \frac{m}{8} \left( 1 - \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma} \right) \right\}$, at least $\frac{m}{2}\left( 1 - \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma} \right) $ of $\rvw_r$ satisfies, $\forall t \le \frac{\tau}{ 2 \eta }$,
\begin{equation*}
	\frac{d\tilde{\ell}_t}{d \rvw_r(t)} = \frac{d \rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t}{d \rvw_r(t)}.
\end{equation*}
\end{lem}
\begin{proof}
Use \cref{lem:gradient_coupling} and \cref{lem:chernoff} with $\alpha = \frac{1}{2}$, $p = 1 - \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma}$, and $\mu = m \left( 1 - \frac{\sqrt{2}\tau}{\sqrt{\pi}\sigma} \right)$.
\end{proof}
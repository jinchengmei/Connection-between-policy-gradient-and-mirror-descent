\section{Conclusions}
\label{sec:conclusions}
We established a $\tilde{O}(T^{2/3})$ regret for a straightforward policy based reinforcement learning algorithm under the stochastic multi-armed bandit setting. 
Noting that the main hurdle in our proof is the estimation error $\|\hat{\rvr} - \rvr\|$, we further proposed a value based RL algorithm, Logit Learning with $\varepsilon$-Greedy Exploration (LLE),
and proved that LLE achieves a nearly optimal regret of $O((\ln T)^2)$.
These results can be generalized to many state dependent bandit settings and episodic MDPs, and can be combined with multi-layered neural network function approximators. Our findings are at the starting point of understanding more perspectives and providing theoretical support for deep reinforcement learning techniques. %We also discussed several open problems for future research.

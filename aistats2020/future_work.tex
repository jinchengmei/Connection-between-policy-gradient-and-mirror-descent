\section{Future Work}
\label{sec:future_work}

Our results are just at the beginning of theoretically understanding the online neural network learning and its combination with reinforcement learning. There are many problems remain open along this line.
\begin{itemize}
    \item \cref{alg:policy_gradient_uniform_exploration} achieves $\tilde{O}(T^{2/3})$ regret. We hypothesize that better rates should be achievable with other strategies. For example, last softmax layer works better with maximum entropy reward \citep{nachum2017bridging}. From this perspective, mirror descent/REINFORCE seem to be natural.
        \item We consider the $\varepsilon$-greedy for exploration. With other type of explorations, like UCB \citep{auer2002finite}, EXP3 \citep{seldin2014one}, and posterior sampling \citep{agrawal2012analysis} can online NN learning and bandit algorithms work well?
    \item Our algorithms use empirical estimation to control functional variation $V_T^f$, which is not scalable for problems in practice. Instead, most practical RL algorithms use sampled reward as an estimation of true reward. However, as mentioned this estimation will have large variation, bringing undesirable learning results. Whether using sampled reward can achieve provable learning results remains open.
    \item In practice, gradient updates always travel a long distance rather than around initialization \citep{liu2018deeptracker}. And usually, practical NNs do not have that much overparametrized scales. Further progresses in the DL theory would be helpful for improving our current analyses of online NN learning and deep reinforcement learning.
    
    \item We investigated the vanilla gradient based algorithms. Combination with other successful RL algorithms including value based and actor critic methods, e.g., DQN \citep{mnih2015human}, A3C \citep{mnih2016asynchronous}, and PCL \citep{nachum2017bridging}, should also be investigated.
    %\item For policy leaning with NN function approximations, there is no known lower bound, although at least the general $\Omega\left(\sqrt{T}\right)$ lower bound holds. We can get some sense from the two parts of the regret in \cref{thm:policy_gradient_main_result}. The NN learning part seems cannot be improved, since this is the best one can obtain under smoothness like properties and gradient upper $\&$ lower bounds. While the other exploration and estimation error part seems still have space to be improved. But whether $\Omega\left(\sqrt{T}\right)$ is achievable is still unknown.
\end{itemize}

\if0
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}
\fi
\section{General Settings}
\label{sec:general_settings}

Although stochastic MAB setting is an oversimplified reinforcement learning environment, it captures trade-off between exploration and exploitation, which is an intrinsic aspect of many more complicated reinforcement learning tasks. Beyond the stochastic MAB, our results can be easily generalized to the following more general settings with minimal effort.

\paragraph{Episodic Markov Decision Processes (MDPs).}

Each action in a standard bandit setting can be viewed as a special trajectory with length $H = 1$ in an episodic MDP. In general with $H > 1$, \cref{alg:logit_learning_eps_greedy_exploration} can achieve similar results, with the total trajectory number changing from $K$ to $K^H$.
\begin{thm}
\label{thm:episodic_mdp_setting}
     Suppose $m \in \Theta\left( \frac{T^2}{c^4 K^{2H}} \right)$, and $\eta = \frac{ 1 }{16 K m}$. The expected regret of \cref{alg:logit_learning_eps_greedy_exploration} satisfies
\begin{equation*}
\scalemath{0.85}{
	\sum\limits_{t=0}^{T-1}{ ( {\rvpi^*} - \tilde{\rvpi}_t )^\top \rvr } \le \sum\limits_{k \in [K^H] : \Delta(k) > 0}{ \left[ \frac{ 3 \beta \left( \ln{T} \right)^2}{\Delta^2(k)} \right] } + 3 \ln{T} + C,
}
\end{equation*} 
where $C$ is independent of $T$.
\end{thm}

The number of terms in the summation has an undesirable exponential dependence on the trajectory length $H$. It seems this cannot be eliminated by policy based RL methods, without taking advantage of intermediate rewards like in value based reinforcement learning methods, such as Q-learning \citep{jin2018q}. While recently there have appeared preliminary analyses of Q-learning \citep{yang2019theoretical,achiam2019towards}, it is still open whether value based RL methods can enjoy nice theoretical guarantees with neural network function approximation.

\paragraph{Contextual Bandit Setting.}

The standard MAB setting is a special case of the state dependent bandit setting, with state number $n = 1$. First, all intermediate results hold for general $n > 0$, including the $n > 1$ case. For $n > 1$ and $\delta \triangleq \min_{i, j \in [n]}{\left\| \rvs_i - \rvs_j \right\|_2} > 0$, we slightly modify \cref{alg:logit_learning_eps_greedy_exploration}, by setting the objective as $\frac{1}{2n}\sum_{i=1}^{n}{ \| \rvo_{i, t} - \hat{\rvr}_{i,t} \|^2}$. Consider the average expected loss over all states, the regret results are similar to \cref{thm:logit_learning_main_result}, up to an additional factor $n$, which can be easily reproduced.
\begin{thm}
\label{thm:many_state_dependent_bandit_setting}
     Suppose $m \in \Theta\left( \frac{n^{10} T^2}{c^4 \delta^4 K^2} \right)$, and $\eta = \frac{c^2 \delta^2}{16 n^4 K m}$. The expected regret of \cref{alg:logit_learning_eps_greedy_exploration} satisfies
\begin{equation*}
\scalemath{0.75}{
	\frac{1}{n } \sum\limits_{i=1}^{n}{ \sum\limits_{t=0}^{T-1}{ ( {\rvpi_i^*} - \tilde{\rvpi}_{i, t})^\top \rvr_i  } \le \sum\limits_{k \in [K] : \Delta_i(k) > 0}{ \left[ \frac{ 3 \beta n^8 \left( \ln{T} \right)^2}{c^4 \delta^4 \Delta_i^2(k)} \right] } + 3 n \ln{T} + C },
}
\end{equation*}
where $C$ is independent of $T$.
\end{thm}



\section{Online Learning with Non-Convex Objectives and Dynamic Regret}
\label{sec:online_learning}

Online learning has been well studied through online convex optimization \citep{shalev2012online}. We consider different assumptions of non-convex, smooth, gradient dominant functions, which are suitable for our problems and algorithms.

\begin{asmp}[Bounded and smooth]
\label{asmp:boundedness_smoothness}
$\left| f_t(\rvx) \right| \le 1$, $\left\| \nabla f_t(\rvx ) - \nabla f_t(\rvy) \right\| \le L $, $\forall \rvx, \rvy$, $\forall t$.
\end{asmp}

\begin{asmp}[Gradient dominant]
\label{asmp:gradient_dominant}
$\left\| \nabla f_t(\rvx) \right\| \ge G \left[ f_t(\rvx) - \min_{\rvw}{ f_t(\rvw )} \right]$, $\forall \rvx$, $\forall t$.
\end{asmp}

\begin{defi}[Functional variation \citep{besbes2015non}]
\label{defi:function_variation}
$V_{T}^{f} \coloneqq \sum_{t=0}^{T-1}{\max\limits_{\rvw}{\left| f_{t+1}(\rvw) - f_{t}(\rvw) \right| } }.$
\end{defi}

Given a sequence of functions $\left\{ f_t : t = 0, 1, \dots, T \right\}$, $V_{T}^{f} $ characterizes how fast/slowly $f_t$ changes over time. For a fast changing sequence with large $V_{T}^{f} $, it is difficult to learn/optimize $f_t$, which is known in online convex optimization literature \citep{jadbabaie2015online}. Under non-convex assumptions, we establish the following result for online gradient update.

\begin{thm}
\label{thm:online_learning_regret}
For any sequence of functions $\left\{ f_t : t = 0, 1, \dots, T \right\}$ that satisfies \cref{asmp:boundedness_smoothness} and \cref{asmp:gradient_dominant}, dynamic regret of online gradient update $\rvx_{t+1} \gets \rvx_{t} - \eta \nabla f_{t}(\rvx_{t})$ with $\eta = 1/L$ satisfies,
\begin{equation}
\label{eq:online_learning_regret}
\begin{split}
    \sum\limits_{t=0}^{T-1}{\left[ f_t(\rvx_t) - f_t( \rvx_t^*) \right]} \le \frac{2}{G} \sqrt{L T ( 1 + V_T^f )},
\end{split}
\end{equation}
where $\rvx_t^* \coloneqq \argmin_{\rvw}{f_t(\rvw)}$, $\forall t$.
\end{thm}

\cref{thm:online_learning_regret} is novel and meaningful as, (1) instead of comparing with best fixed decision in hindsight $\min_{\rvw}{\sum_{t=0}^{T-1}{f_t(\rvw)} }$, we consider changing optimal solutions $f_t\left( \rvx_t^* \right)$, which is more difficult to upper bound; (2) unlike most existing work, here $f_t$ is not assumed to be convex \citep{yang2016tracking} or quasi-convex \citep{gao2018online}; (3) we analyze practical gradient update, rather than oracle based methods \citep{suggala2019online,agarwal2019learning} ; (4) our assumptions are satisfied by randomly initialized NNs \citep{li2018learning,allen2018convergenceB}.

To obtain sub-linear regret in \cref{eq:online_learning_regret}, it is sufficient to control the functional variation $V_T^f$ to be sub-linear $o(T)$. However, $V_T^f$ is usually linear $O(T)$ in practice. For example, in stochastic MAB, if we take sampled reward as learning signal, then at each step signal is changing in order $O(1)$ according to random sampling. As a result, $V_T^f$ will be $O(T)$. Therefore, we use empirical mean estimation and exploration to control $V_T^f$.

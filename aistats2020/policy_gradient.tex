\section{Empirical Policy Gradient}
\label{sec:policy_gradient}

\cref{thm:online_learning_regret} shows that the variation of learning objectives matters when gradient based methods are used in online learning. In stochastic bandit settings, to characterize similar issues when different objectives are used for learning, we define a special functional variation, called \textbf{reward signal variation}.

\begin{defi}[Reward signal variation]
\label{defi:reward_signal_variation}
Given a sequence of reward signals $\{ \tilde{\rvr}_t \in [0,1]^K: t = 0, 1, 2, \dots, T-1 \}$, define reward signal variation
$V_{T}^{\tilde{\rvr}} \coloneqq \sum_{t=0}^{T-1}{\max\limits_{\rvpi \in \Delta}{\left| \rvpi^\top(\tilde{\rvr}_{t+1} - \tilde{\rvr}_{t}) \right| } }.$
\end{defi}

Using the following example, we show different reward signal has significantly different variations.

\begin{eg}
Consider a true mean reward scalar $r \in [0, 1]$, and its two observed samples $r_t, r_{t+1} \in [0, 1]$. $\left| r_t - r_{t+1} \right| \in O(1)$ is independent with $t$, and it leads to $V_T^r \in O(T)$. On the other hand, consider empirical mean $\hat{r}_t, \hat{r}_{t+1} \in [0, 1]$, and
\begin{equation*}
\begin{split}
	\left| \hat{r}_{t} - \hat{r}_{t+1} \right| &\coloneqq \left| \frac{1}{t} \sum_{i=1}^{t}{ r_i } - \frac{1}{t+1} \sum_{i=1}^{t+1}{ r_i } \right| \\
	&= \frac{1}{t+1} \left| \hat{r}_t - r_{t+1} \right| \in O\left(\frac{1}{t}\right)
\end{split}
\end{equation*}
decays as $t$ increases, which leads to $V_T^{\hat{r}} \in O(\log{T})$.
\end{eg}

This simple example demonstrates that empirical mean with enough exploration has sub-linear reward signal variation $V_T^{\hat{r}} \in o(T)$, and it is  much more stable than sampled reward. Inspired by this intuition of controlling reward signal variation $V_T^{\tilde{\rvr}}$, we propose our first algorithm, Empirical Policy Gradient with Uniform Exploration (PGE), as shown in \cref{alg:policy_gradient_uniform_exploration}. Following widely used policy gradient methods, PGE updates policy neural networks by policy gradient update, but with empirical reward estimation $\hat{\rvr}_t$ as its objective. At each step $t$, PGE also has a uniform exploration $t^{ \beta - \frac{1}{3}} \cdot \frac{1}{K}$ for each action, with $\beta > 0$ to be very small. There are two main contrasts between PGE and standard policy gradient methods, i.e., the later use sampled reward as learning objective and have no explicit exploration. Actually, empirical mean and exploration are key points for PGE to control $V_T^{\hat{\rvr}}$. 
 
Based on this idea, PGE achieves $\tilde{O}(T^{2/3})$ regret by balancing exploration, estimation, and online neural network learning, as shown in \cref{thm:policy_gradient_main_result}.

\begin{algorithm}[t]
\caption{Empirical Policy Gradient with Uniform Exploration (PGE)}
\label{alg:policy_gradient_uniform_exploration}
\begin{algorithmic}
   \STATE {\bfseries Input:} State feature $\rvs$, learning rate $\eta > 0$, $\beta > 0$.
   \STATE $\rvw_r(0) \sim \gN\left( 0, \sigma^2 \rmI \right)$, $\forall r \in [m]$. $a_{k, r} \sim 
   \unif\left\{-1, +1\right\}$, $\forall k \in [K]$, $\forall r \in [m]$.
   \STATE $\hat{r}_{0}(k) \gets 0$, $n_{0}(k) \gets 0$, $\tilde{\pi}_0(k) \gets \frac{1}{K}$, $\forall k \in [K]$.
   \FOR{$t=0$ {\bfseries to} $T-1$}
   \STATE Sample $A_{t} \sim \tilde{\rvpi}_{t}(\cdot | \rvs )$. Take $A_{t}$. Get reward $R_{t}$.
   \STATE $\rmW_{t+1} \leftarrow \rmW_t + \eta \cdot \frac{d \{ \rvpi_t^\top \hat{\rvr}_t \} }{d \rmW_t}$.
   \STATE $\scalemath{0.8}{ \left. 
		\begin{cases}
		n_{t+1}(k) \gets  n_{t}(k) + 1, \ \hat{r}_{t+1}(k) \gets \frac{n_{t}(k) \cdot \hat{r}_{t}(k) + R_{t} }{n_{t+1}(k)},  & \text{if } k = A_t, \\
		n_{t+1}(k) \gets n_{t}(k), \ \hat{r}_{t+1}(k) \gets \hat{r}_{t}(k),  & \text{otherwise}.
		\end{cases}
		\right.}$ 
   \STATE $\scalemath{0.75}{ \tilde{\pi}_{t+1}(k) \gets \left[ 1 - (t+1)^{ \beta - \frac{1}{3} } \right] \cdot  \pi_{t+1}(k) + (t+1)^{ \beta - \frac{1}{3} }  \cdot \frac{1}{K}, \ \forall k \in [K] }$.
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{thm}
	\label{thm:policy_gradient_main_result}
	Given policy neural networks as shown in \cref{fig:nn_policy_value}, with number of parameters $m \ge T^2 / K^2$, learning rate $\eta = \frac{1}{2 K m}$, $\beta = \frac{ \ln{(K/3) + \ln{\ln{t}} } }{ 3 \ln{t}}$, $\forall t \ge 2$. The expected regret of \cref{alg:policy_gradient_uniform_exploration} satisfies,
	\begin{equation*}
	\begin{split}
	\sum\limits_{t=0}^{T-1}{ ( \rvpi^* - \tilde{\rvpi}_t )^\top \rvr} \le O(T^{\frac{2}{3}}( \ln{T} )^{\frac{1}{3}}).
	\end{split}
	\end{equation*}
\end{thm}
\begin{proof}[\bf Proof sketch]
The sampling policy $\tilde{\rvpi}_t$ of PGE consists of network policy $\rvpi_t$ and uniform exploration. Therefore, we decompose regret into two parts,
\begin{equation}
 \label{eq:total_regret_decomposition}
 \scalemath{0.9}{
 \begin{split}
 \sum\limits_{t=0}^{T-1}{ ( \rvpi^* - \tilde{\rvpi}_t )^\top \rvr} \le \underbrace{ 2 T^{\frac{2}{3}} ( K \ln{T} )^{\frac{1}{3}} }_{\text{exploration cost}} + \underbrace{ \sum\limits_{t=1}^{T-1}{ ( {\rvpi^*} - \rvpi_t)^\top \rvr } }_{\text{NN regret}},
 \end{split}
 }
 \end{equation}
where the exploration cost is by summing $(t+1)^{ \beta - \frac{1}{3} }  \cdot \frac{1}{K}$. Also, because of the exploration, for large enough $t$, with high probability, $\hat{\rvr}_t$ will accurately estimate $\rvr$,
\begin{equation*}
    \| \hat{\rvr}_t - \rvr \|_\infty \le t^{\beta - \frac{1}{3}} \le ( K \ln{t} ) ^\frac{1}{3} t^{- \frac{1}{3}}.
\end{equation*}
The NN regret can also be decomposed as
\begin{equation}
\label{eq:playing_learning_phase_regret_decomposition}
\scalemath{0.78}{
\begin{split}
\sum\limits_{t=1}^{T-1}{ ( {\rvpi^*} - \rvpi_t )^\top \rvr}  &= \sum\limits_{t=1}^{T-1}{ ( {\rvpi^*} - \rvpi_t )^\top \hat{\rvr}_t } + \sum\limits_{t=1}^{T-1}{ ( {\rvpi^*} - \rvpi_t )^\top ( \rvr - \hat{\rvr}_t ) }\\
&\le \sum\limits_{t=1}^{T-1}{ ( {\rvpi_t^*} - \rvpi_t )^\top \hat{\rvr}_t  } + 2 \sum\limits_{t=1}^{T-1}{ \| \rvr - \hat{\rvr}_t \|_\infty } \\
&\le \underbrace{ \sum\limits_{t=1}^{T-1}{ ( {\rvpi_t^*} - \rvpi_t )^\top \hat{\rvr}_t  } }_{\text{dynamic NN regret}} + \underbrace{ O(T^{\frac{2}{3}}( \ln{T} )^{\frac{1}{3}}) }_{\text{estimation error}},
\end{split}
}
\end{equation}
where $\rvpi_t^* \coloneqq \argmax_{\rvpi }{\{ \rvpi^\top \hat{\rvr}_t\}}$. The first inequality is by $( \rvpi_t^* - \rvpi^*)^\top \hat{\rvr}_t \ge 0$ and H{\" o}lder's inequality. Next, the key step is to prove that if $m \ge T^2 /K^2$, $\eta = \frac{1}{2 K m}$, then the dynamic NN regret satisfies,
\begin{equation}
\label{step:proof1step1}
\begin{split}
\sum\limits_{t=1}^{T-1}{ (  {\rvpi_t^*} - \rvpi_t )^\top \hat{\rvr}_t } \le \frac{7 K}{c} \cdot  T^{\frac{2}{3}}
\end{split}
\end{equation}
with probability at least $1 - \frac{16 K}{T^2}$.
The regret bound follows by combining \cref{eq:total_regret_decomposition}, \cref{eq:playing_learning_phase_regret_decomposition}, and \cref{step:proof1step1}.
\end{proof}
The dynamic regret upper bound of online neural network learning \cref{step:proof1step1} is key to the proof for \cref{thm:policy_gradient_main_result}. Our intuition behind \cref{step:proof1step1} is, \textit{first}, we use exploration to make $\hat{\rvr}_t$ achieve sub-linear variation. In particular, with high probability, for large enough $t$, every action will be visited $\Omega(t^{\frac{2}{3}+ \beta})$ times, which leads to $\| \hat{\rvr}_t  - \hat{\rvr}_{t+1} \|_1 \le O(t^{-\frac{2}{3} - \beta})$. As a result, $V_T^{\hat{\rvr}} \in O(T^{\frac{1}{3}})$; \textit{second}, we observe that expected reward satisfies \cref{asmp:boundedness_smoothness} and \cref{asmp:gradient_dominant}, and they are also satisfied with high probability depending on parameter number of neural networks, according to recently developed NN optimization theory \citep{li2018learning,allen2018convergenceB}. Applying \cref{thm:online_learning_regret}, the dynamic NN regret in \cref{step:proof1step1} is $O\left(\sqrt{T \cdot V_T^{\hat{\rvr}} }\right) \in O(T^{\frac{2}{3}})$.

\begin{remk}
The three parts, i.e., exploration cost, estimation error, dynamic regret for online neural network learning, in the proof for \cref{thm:policy_gradient_main_result} are almost matching with each other at order $\tilde{O}(T^{2/3})$, which is worse than $\Omega(\sqrt{T})$ lower bound. However, gradient based methods with function approximations may have lower bound higher than general $\Omega(\sqrt{T})$. Whether our result is optimal is under investigation.
\end{remk}

\iffalse
\subsection{Exploration of the Optimal Action}
\label{subsec:exploration_in_policy_learning}

Consider the logit derivative of the true mean reward,
\begin{equation}
\label{eq:logit_derivative}
\begin{split}
    \frac{d \rvpi( \rvo)^\top \rvr}{d \rvo} = \left[ \Delta(\rvpi) - \rvpi \rvpi^\top \right] \rvr,
\end{split}
\end{equation}
where $\Delta(\rvpi) \in \sR^{h \times h}$ is a diagonal matrix with $\rvpi$ as its diagonal. Suppose the $k$th action is worth learning, i.e, $r(k) - \rvpi^\top \rvr > 0$ is large, meaning this action has mean reward $r(k)$ much larger than the expected mean reward of the current policy, so the agent should increase its action logit. But if $\pi(k)$ is very close to zero, the increase of the $k$th action logit will be small. Since $\pi(k)$ is small, the $k$th action will be sampled rarely, and with other action logits increasing, $\pi(k)$ will be even smaller, which makes eventually the $k$th action cannot be sampled and learned any more.

\cref{eq:logit_derivative} indicates that to learn the $k$th action, $\pi_{t}(k) > c > 0$ should hold for some constant $c$. In particular, to learn an optimal policy, $\pi_{t}(k^*) > c > 0$ should be guaranteed, $\forall t \ge 0$, where $r(k^*) = \max\limits_{k \in \left[ h \right]}\left\{ r(k) \right\}$.

Consider policy update using \cref{eq:logit_derivative}. $\forall \eta > 0$, $\forall k \in [h]$,
\begin{equation*}
\label{eq:logit_increment_logit_space}
\begin{split}
\small
    o_{t+1}(k) - o_{t}(k) = \eta \cdot \pi_{t}(k) \cdot \left( r(k) - \rvpi(\rvo_{t})^\top \rvr \right),
\end{split}
\end{equation*}
which mean as long as $\pi_{t}(k) > 0$, for any valuable action $k$ with $r(k) -  \rvpi(\rvo_{t})^\top \rvr > 0$, the action logit will increase. While for any bad actions with $r(k) -  \rvpi(\rvo_{t})^\top \rvr < 0$, their logits will decrease.

Consider the uniform policy $\rvpi( \rvo_0)$, with $\pi_{0}(k^*) = \frac{1}{h} \in \Omega(1)$. Also note $r(k^*) - \rvpi( \rvo_0)^\top \rvr$ is larger than any other action $k$, because $k^*$ is the the optimal action. Therefore, the optimal action logit will have the largest positive increment than all the other suboptimal actions. After the softmax transform, the optimal action probability will be larger than its previous value. 
\begin{lem}
\label{lem:optimal_probability_increse_logit_space}
Policy update using \cref{eq:logit_derivative} satisfies, $\forall t \ge 0$,
\begin{equation*}
    \pi_{t+1}(k^*) \ge \pi_{t}(k^*) \in \Omega(1).
\end{equation*}
\end{lem}

\cref{alg:policy_gradient_uniform_exploration} enjoys similar results with \cref{lem:optimal_probability_increse_logit_space}. First, with enough exploration, $\hat{\rvr}_t$ is close to $\rvr$, thus $\rvpi\left(\rmW_t\right)^\top \hat{\rvr}_t$ is close to $\rvpi\left(\rmW_t\right)^\top \rvr$. Second, $\pi_{0}(\hat{k}_t^*) \approx \frac{1}{h} \in \Omega(1)$. Third, since the policy gradient update is around the initialization by \cref{lem:gradient_coupling}, the policy gradient updates behave similarly with the logit derivative updates, which makes the the optimal action logit always large comparing with the suboptimal actions.
\begin{lem}
\label{lem:optimal_probability_increse_parameter_space}
\cref{alg:policy_gradient_uniform_exploration} satisfies, $\forall t \ge 0$,
\begin{equation*}
    \pi_{t}(\hat{k}_t^*) \in \Omega(1).
\end{equation*}
\end{lem}
\cref{lem:optimal_probability_increse_parameter_space} indicates replacing $c$ in \cref{thm:policy_gradient_main_result} will not incur any additional regret dependent on $T$.
\fi
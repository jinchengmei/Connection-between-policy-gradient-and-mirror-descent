\section{Connections}

One common way to get an unbiased estimation of $\rvr_t$ is to use importance sampling. At step $t$, sample an action $A_t \sim \rvpi_t$, and construct the estimator as
\begin{equation*}
    \rvone_{ \left\{ \frac{\rvr_t(A_t) }{\rvpi_t(A_t)}  \right\} } \triangleq \begin{cases}
		\frac{\rvr_t(A_t) }{\rvpi_t(A_t)}, & \text{if } i = A_t, \\
		0, & \text{otherwise}.
		\end{cases}
\end{equation*}
This estimator is unbiased
\begin{equation*}
    \expectation\limits_{A_t \sim \rvpi_t}{ \rvone_{ \left\{ \frac{\rvr_t(A_t) }{\rvpi_t(A_t)}  \right\} } } = \sum\limits_{ i \in [K] }{ \rvpi_t(i) \cdot \rvone_{ \left\{ \frac{\rvr_t(i) }{\rvpi_t(i)}  \right\} }  } = \sum\limits_{i \in [K] }{ \rvone_{ \left\{ \rvr_t(i)  \right\} } } = \rvr_t.
\end{equation*}
We first show that when the importance sampling estimator is used, \cref{alg:erpg_ment_reinforce} has exactly the same update form as the classic MENT/REINFORCE algorithm.

\begin{lem}
\label{lem:equivalence_ment_reinforce}
Using the above importance sampling in \cref{alg:erpg_ment_reinforce},
\begin{equation*}
    \nabla{(\rvtheta_t)} = \expectation\limits_{\rho \sim \rvpi_t}{ \left( \rvr_t(\rho) - \log{\rvpi_t(\rho)} \right) \cdot \nabla{ \log{\rvpi_t(\rho)} } }, \quad \forall t \ge 1.
\end{equation*}
\end{lem}